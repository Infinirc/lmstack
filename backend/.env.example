# LMStack Backend Configuration
# Copy this file to .env and update the values as needed

# =============================================================================
# Application Settings
# =============================================================================

# Application name (default: LMStack)
# LMSTACK_APP_NAME=LMStack

# Debug mode - enables verbose logging and hot reload (default: false)
# WARNING: Do not enable in production
# LMSTACK_DEBUG=false

# =============================================================================
# Database Configuration
# =============================================================================

# Database URL (default: sqlite+aiosqlite:///./lmstack.db)
# For PostgreSQL: postgresql+asyncpg://user:password@localhost/lmstack
# For MySQL: mysql+aiomysql://user:password@localhost/lmstack
LMSTACK_DATABASE_URL=sqlite+aiosqlite:///./lmstack.db

# =============================================================================
# Server Configuration
# =============================================================================

# Server host (default: 0.0.0.0)
# LMSTACK_HOST=0.0.0.0

# Server port (default: 8000)
# LMSTACK_PORT=8000

# CORS origins - comma-separated list of allowed origins
# Use "*" for development only, specify exact origins for production
# Example: http://localhost:3000,http://localhost:5173,https://your-domain.com
LMSTACK_CORS_ORIGINS=*

# =============================================================================
# Authentication
# =============================================================================

# JWT secret key for token signing
# IMPORTANT: Generate a secure random key for production!
# You can generate one with: python -c "import secrets; print(secrets.token_hex(32))"
# LMSTACK_SECRET_KEY=your-secret-key-here

# JWT token expiration time in minutes (default: 1440 = 24 hours)
# LMSTACK_ACCESS_TOKEN_EXPIRE_MINUTES=1440

# =============================================================================
# Worker Configuration
# =============================================================================

# Worker heartbeat interval in seconds (default: 30)
# LMSTACK_WORKER_HEARTBEAT_INTERVAL=30

# Worker timeout in seconds - mark offline if no heartbeat (default: 90)
# LMSTACK_WORKER_TIMEOUT=90

# =============================================================================
# Model Inference Backend Images
# =============================================================================

# vLLM Docker image (default: vllm/vllm-openai:latest)
# LMSTACK_VLLM_DEFAULT_IMAGE=vllm/vllm-openai:latest

# SGLang Docker image (default: lmsysorg/sglang:latest)
# LMSTACK_SGLANG_DEFAULT_IMAGE=lmsysorg/sglang:latest

# Ollama Docker image (default: ollama/ollama:latest)
# LMSTACK_OLLAMA_DEFAULT_IMAGE=ollama/ollama:latest

# Ollama host port (default: 11434)
# LMSTACK_OLLAMA_HOST_PORT=11434

# =============================================================================
# Storage Configuration
# =============================================================================

# HuggingFace model cache directory
# This path is mounted into Docker containers for model caching
# Default: /root/.cache/huggingface
LMSTACK_HF_CACHE_DIR=/root/.cache/huggingface

# Data directory for application data (default: ./data)
# LMSTACK_DATA_DIR=./data
